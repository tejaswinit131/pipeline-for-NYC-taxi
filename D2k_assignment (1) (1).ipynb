{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebec255d-d765-4ac8-85d0-99424b8360ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install tqdm pandas requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d796dcd1-f93c-4afa-aa51-7bcb6c9e086f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06eb1fa4-d044-4420-a85d-40a7e74c0b66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9682ac50-a2e0-44a6-b074-acf5d4601451",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "\n",
    "# Create a session to handle retries\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "# Define the base URL for scraping and the local directory to save the files\n",
    "scrape_url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "local_dir = \"/tmp/nyc_taxi_data_2019\"  # Local temporary storage in Databricks\n",
    "\n",
    "# Create the local directory if it doesn't exist\n",
    "os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "# Function to download a file with retries\n",
    "def download_file(url, output_path):\n",
    "    try:\n",
    "        response = session.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        with open(output_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "        print(f\"Successfully downloaded {output_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download {url}: {e}\")\n",
    "\n",
    "# Scrape the website to find the correct URLs\n",
    "response = session.get(scrape_url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "links = soup.find_all('a', href=True)\n",
    "\n",
    "# Filter and download parquet files for the year 2019\n",
    "parquet_files = []\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    if '2019' in href and href.endswith('.parquet'):\n",
    "        filename = href.split('/')[-1]\n",
    "        url = href\n",
    "        output_path = os.path.join(local_dir, filename)\n",
    "        download_file(url, output_path)\n",
    "        parquet_files.append(output_path)\n",
    "        sleep(1)  # Sleep for a short time between requests to avoid overloading the server\n",
    "\n",
    "print(\"Download completed.\")\n",
    "\n",
    "# Copy the downloaded files to DBFS\n",
    "dbfs_dir = \"/mnt/nyc_taxi_data_2019\"  # Change to your desired DBFS path\n",
    "\n",
    "# Create the DBFS directory if it doesn't exist\n",
    "dbutils.fs.mkdirs(dbfs_dir)\n",
    "\n",
    "# Copy files to DBFS\n",
    "for local_file in parquet_files:\n",
    "    dbfs_file = os.path.join(dbfs_dir, os.path.basename(local_file))\n",
    "    dbutils.fs.cp(f\"file:{local_file}\", dbfs_file)\n",
    "    print(f\"Copied {local_file} to {dbfs_file}\")\n",
    "\n",
    "print(\"Files copied to DBFS.\")\n",
    "\n",
    "# Read and display data from each parquet file in DBFS using Spark\n",
    "for local_file in parquet_files:\n",
    "    dbfs_file = os.path.join(dbfs_dir, os.path.basename(local_file))\n",
    "    df = spark.read.parquet(dbfs_file)\n",
    "    df.show(5)\n",
    "\n",
    "print(\"Data loaded and displayed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7dc4b94-ea36-4eb0-96b6-e517e611c4ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "def clean_and_transform_spark(df, taxi_type):\n",
    "    if taxi_type == \"yellow\":\n",
    "        required_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df = df.withColumn('pickup_datetime', F.to_timestamp('tpep_pickup_datetime')) \\\n",
    "               .withColumn('dropoff_datetime', F.to_timestamp('tpep_dropoff_datetime'))\n",
    "    elif taxi_type == \"green\":\n",
    "        required_columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df = df.withColumn('pickup_datetime', F.to_timestamp('lpep_pickup_datetime')) \\\n",
    "               .withColumn('dropoff_datetime', F.to_timestamp('lpep_dropoff_datetime'))\n",
    "    elif taxi_type == \"hvfht\":\n",
    "        required_columns = ['pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare']\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df = df.withColumn('pickup_datetime', F.to_timestamp('pickup_datetime')) \\\n",
    "               .withColumn('dropoff_datetime', F.to_timestamp('dropoff_datetime'))\n",
    "    \n",
    "    df = df.withColumn('trip_duration', (F.col('dropoff_datetime').cast('long') - F.col('pickup_datetime').cast('long')) / 60) \\\n",
    "           .withColumn('average_speed', F.col('trip_distance') / (F.col('trip_duration') / 60))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Read, clean, and display data from each parquet file in DBFS using Spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "for local_file in parquet_files:\n",
    "    dbfs_file = os.path.join(dbfs_dir, os.path.basename(local_file))\n",
    "    taxi_type = \"\"\n",
    "    if \"yellow\" in dbfs_file:\n",
    "        taxi_type = \"yellow\"\n",
    "    elif \"green\" in dbfs_file:\n",
    "        taxi_type = \"green\"\n",
    "    elif \"hvfht\" in dbfs_file:\n",
    "        taxi_type = \"hvfht\"\n",
    "    \n",
    "    if taxi_type:\n",
    "        df = spark.read.parquet(dbfs_file)\n",
    "        df = clean_and_transform_spark(df, taxi_type)\n",
    "        df.show(5)\n",
    "\n",
    "print(\"Data loaded, cleaned, and displayed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44648b3-81db-4b82-bfe6-0392cb94bdb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stderr:\u001B[0m\n",
       "\u001B[0;31mSun Jul 14 17:20:18 2024 Connection to spark from PID  1397\u001B[0m\n",
       "\u001B[0;31mSun Jul 14 17:20:18 2024 Initialized gateway on port 36775\u001B[0m\n",
       "\u001B[0;31mSun Jul 14 17:20:18 2024 Connected to spark.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stdout:\u001B[0m\n",
       "\u001B[0;31m      2.5|  0.0|    0.5|       0.0|         0.0|                  0.3|         3.3|                 0.0|       null|2019-11-01 00:18:30|2019-11-01 00:18:39|               0.15|          0.0|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n",
       "\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|    pickup_datetime|   dropoff_datetime|     trip_duration|     average_speed|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n",
       "\u001B[0;31m       2| 2019-11-01 00:11:24|  2019-11-01 00:23:12|                 N|       1.0|          66|         148|            1.0|          2.8|       11.5|  0.5|    0.5|      3.11|         0.0|     null|                  0.3|       18.66|         1.0|      1.0|                2.75|2019-11-01 00:11:24|2019-11-01 00:23:12|              11.8|14.237288135593218|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-11-01 00:49:25|  2019-11-01 01:14:19|                 N|       1.0|         145|         114|            1.0|         5.59|       20.0|  0.5|    0.5|      6.03|        6.12|     null|                  0.3|        36.2|         1.0|      1.0|                2.75|2019-11-01 00:49:25|2019-11-01 01:14:19|              24.9| 13.46987951807229|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-11-01 00:57:22|  2019-11-01 01:09:23|                 N|       1.0|         255|          37|            1.0|          2.1|       10.0|  0.5|    0.5|      2.25|         0.0|     null|                  0.3|       13.55|         1.0|      1.0|                 0.0|2019-11-01 00:57:22|2019-11-01 01:09:23|12.016666666666667|10.485436893203882|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-11-01 00:59:52|  2019-11-01 01:08:19|                 N|       1.0|           7|         226|            1.0|         1.23|        7.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.8|         2.0|      1.0|                 0.0|2019-11-01 00:59:52|2019-11-01 01:08:19|              8.45| 8.733727810650889|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-11-01 00:40:13|  2019-11-01 00:47:41|                 N|       1.0|         129|         129|            1.0|         1.18|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|         2.0|      1.0|                 0.0|2019-11-01 00:40:13|2019-11-01 00:47:41| 7.466666666666667| 9.482142857142858|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-----------------+\u001B[0m\n",
       "\u001B[0;31mVendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|    pickup_datetime|   dropoff_datetime|      trip_duration|    average_speed|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-----------------+\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:26:58|  2019-12-01 00:41:45|            1.0|          4.2|       1.0|                 N|         142|         116|           2|       14.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        18.3|                 2.5|       null|2019-12-01 00:26:58|2019-12-01 00:41:45| 14.783333333333333|17.04622322435175|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:12:08|  2019-12-01 00:12:14|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|2019-12-01 00:12:08|2019-12-01 00:12:14|                0.1|              0.0|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:25:53|  2019-12-01 00:26:04|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|2019-12-01 00:25:53|2019-12-01 00:26:04|0.18333333333333332|              0.0|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:12:03|  2019-12-01 00:33:19|            2.0|          9.4|       1.0|                 N|         138|          25|           1|       28.5|  0.5|    0.5|      10.0|         0.0|                  0.3|        39.8|                 0.0|       null|2019-12-01 00:12:03|2019-12-01 00:33:19| 21.266666666666666|26.52037617554859|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:05:27|  2019-12-01 00:16:32|            2.0|          1.6|       1.0|                 N|         161|         237|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       null|2019-12-01 00:05:27|2019-12-01 00:16:32| 11.083333333333334|8.661654135338345|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-----------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n",
       "\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|    pickup_datetime|   dropoff_datetime|     trip_duration|     average_speed|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:09:45|  2019-12-01 00:10:59|                 N|       1.0|         145|         145|            1.0|          0.0|        3.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         4.3|         2.0|      1.0|                 0.0|2019-12-01 00:09:45|2019-12-01 00:10:59|1.2333333333333334|               0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:26:05|  2019-12-01 00:31:30|                 N|       1.0|          24|          41|            1.0|         0.67|        5.5|  0.5|    0.5|      2.04|         0.0|     null|                  0.3|        8.84|         1.0|      1.0|                 0.0|2019-12-01 00:26:05|2019-12-01 00:31:30| 5.416666666666667|7.4215384615384625|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:38|                 N|       1.0|          74|          41|            1.0|         0.61|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|2019-12-01 00:56:36|2019-12-01 00:59:38| 3.033333333333333|12.065934065934066|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:26:20|  2019-12-01 00:40:19|                 N|       1.0|         255|         157|            1.0|          3.9|       14.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|2019-12-01 00:26:20|2019-12-01 00:40:19|13.983333333333333|16.734207389749702|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:56|                 N|       1.0|          80|         255|            1.0|          0.5|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|2019-12-01 00:56:36|2019-12-01 00:59:56|3.3333333333333335|               9.0|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mData loaded, cleaned, and displayed.\u001B[0m"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stderr:\u001B[0m\n\u001B[0;31mSun Jul 14 17:20:18 2024 Connection to spark from PID  1397\u001B[0m\n\u001B[0;31mSun Jul 14 17:20:18 2024 Initialized gateway on port 36775\u001B[0m\n\u001B[0;31mSun Jul 14 17:20:18 2024 Connected to spark.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stdout:\u001B[0m\n\u001B[0;31m      2.5|  0.0|    0.5|       0.0|         0.0|                  0.3|         3.3|                 0.0|       null|2019-11-01 00:18:30|2019-11-01 00:18:39|               0.15|          0.0|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|    pickup_datetime|   dropoff_datetime|     trip_duration|     average_speed|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n\u001B[0;31m       2| 2019-11-01 00:11:24|  2019-11-01 00:23:12|                 N|       1.0|          66|         148|            1.0|          2.8|       11.5|  0.5|    0.5|      3.11|         0.0|     null|                  0.3|       18.66|         1.0|      1.0|                2.75|2019-11-01 00:11:24|2019-11-01 00:23:12|              11.8|14.237288135593218|\u001B[0m\n\u001B[0;31m       2| 2019-11-01 00:49:25|  2019-11-01 01:14:19|                 N|       1.0|         145|         114|            1.0|         5.59|       20.0|  0.5|    0.5|      6.03|        6.12|     null|                  0.3|        36.2|         1.0|      1.0|                2.75|2019-11-01 00:49:25|2019-11-01 01:14:19|              24.9| 13.46987951807229|\u001B[0m\n\u001B[0;31m       1| 2019-11-01 00:57:22|  2019-11-01 01:09:23|                 N|       1.0|         255|          37|            1.0|          2.1|       10.0|  0.5|    0.5|      2.25|         0.0|     null|                  0.3|       13.55|         1.0|      1.0|                 0.0|2019-11-01 00:57:22|2019-11-01 01:09:23|12.016666666666667|10.485436893203882|\u001B[0m\n\u001B[0;31m       2| 2019-11-01 00:59:52|  2019-11-01 01:08:19|                 N|       1.0|           7|         226|            1.0|         1.23|        7.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.8|         2.0|      1.0|                 0.0|2019-11-01 00:59:52|2019-11-01 01:08:19|              8.45| 8.733727810650889|\u001B[0m\n\u001B[0;31m       2| 2019-11-01 00:40:13|  2019-11-01 00:47:41|                 N|       1.0|         129|         129|            1.0|         1.18|        7.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         8.3|         2.0|      1.0|                 0.0|2019-11-01 00:40:13|2019-11-01 00:47:41| 7.466666666666667| 9.482142857142858|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-----------------+\u001B[0m\n\u001B[0;31mVendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|    pickup_datetime|   dropoff_datetime|      trip_duration|    average_speed|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-----------------+\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:26:58|  2019-12-01 00:41:45|            1.0|          4.2|       1.0|                 N|         142|         116|           2|       14.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        18.3|                 2.5|       null|2019-12-01 00:26:58|2019-12-01 00:41:45| 14.783333333333333|17.04622322435175|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:12:08|  2019-12-01 00:12:14|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|2019-12-01 00:12:08|2019-12-01 00:12:14|                0.1|              0.0|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:25:53|  2019-12-01 00:26:04|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|2019-12-01 00:25:53|2019-12-01 00:26:04|0.18333333333333332|              0.0|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:12:03|  2019-12-01 00:33:19|            2.0|          9.4|       1.0|                 N|         138|          25|           1|       28.5|  0.5|    0.5|      10.0|         0.0|                  0.3|        39.8|                 0.0|       null|2019-12-01 00:12:03|2019-12-01 00:33:19| 21.266666666666666|26.52037617554859|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:05:27|  2019-12-01 00:16:32|            2.0|          1.6|       1.0|                 N|         161|         237|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       null|2019-12-01 00:05:27|2019-12-01 00:16:32| 11.083333333333334|8.661654135338345|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+-------------------+-------------------+-------------------+-----------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|    pickup_datetime|   dropoff_datetime|     trip_duration|     average_speed|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:09:45|  2019-12-01 00:10:59|                 N|       1.0|         145|         145|            1.0|          0.0|        3.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         4.3|         2.0|      1.0|                 0.0|2019-12-01 00:09:45|2019-12-01 00:10:59|1.2333333333333334|               0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:26:05|  2019-12-01 00:31:30|                 N|       1.0|          24|          41|            1.0|         0.67|        5.5|  0.5|    0.5|      2.04|         0.0|     null|                  0.3|        8.84|         1.0|      1.0|                 0.0|2019-12-01 00:26:05|2019-12-01 00:31:30| 5.416666666666667|7.4215384615384625|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:38|                 N|       1.0|          74|          41|            1.0|         0.61|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|2019-12-01 00:56:36|2019-12-01 00:59:38| 3.033333333333333|12.065934065934066|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:26:20|  2019-12-01 00:40:19|                 N|       1.0|         255|         157|            1.0|          3.9|       14.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|2019-12-01 00:26:20|2019-12-01 00:40:19|13.983333333333333|16.734207389749702|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:56|                 N|       1.0|          80|         255|            1.0|          0.5|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|2019-12-01 00:56:36|2019-12-01 00:59:56|3.3333333333333335|               9.0|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+-------------------+-------------------+------------------+------------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mData loaded, cleaned, and displayed.\u001B[0m",
       "errorSummary": "<span class='ansi-red-fg'>Fatal error</span>: The Python kernel is unresponsive.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, filename='data_processing.log', \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def clean_and_transform(df, file_type):\n",
    "    try:\n",
    "        if file_type == 'yellow':\n",
    "            required_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "            df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "            df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "        elif file_type == 'green':\n",
    "            required_columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "            df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "            df['dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "        elif file_type == 'hvfht':\n",
    "            required_columns = ['pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare']\n",
    "        else:\n",
    "            logging.error(f\"Unknown file type: {file_type}\")\n",
    "            return None, None\n",
    "        \n",
    "        for col in required_columns:\n",
    "            if col not in df.columns:\n",
    "                logging.error(f\"Missing column {col} in {file_type} taxi data\")\n",
    "                return None, None\n",
    "        \n",
    "        # Remove trips with missing or corrupt data\n",
    "        df.dropna(subset=required_columns, inplace=True)\n",
    "        \n",
    "        if file_type in ['yellow', 'green']:\n",
    "            df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "            df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "            fare_column = 'fare_amount'\n",
    "        else:\n",
    "            df['trip_duration'] = (pd.to_datetime(df['dropoff_datetime']) - pd.to_datetime(df['pickup_datetime'])).dt.total_seconds() / 60\n",
    "            df['average_speed'] = df['trip_miles'] / (df['trip_duration'] / 60)\n",
    "            fare_column = 'base_passenger_fare'\n",
    "        \n",
    "        # Aggregate data\n",
    "        df['date'] = df['pickup_datetime'].dt.date\n",
    "        daily_aggregates = df.groupby('date').agg({\n",
    "            'trip_duration': 'count',\n",
    "            fare_column: 'mean'\n",
    "        }).rename(columns={'trip_duration': 'total_trips', fare_column: 'average_fare'})\n",
    "        \n",
    "        return df, daily_aggregates\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {file_type} taxi data: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def process_all_files(data_folder):\n",
    "    processed_data = []\n",
    "    daily_aggregates = []\n",
    "    \n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        if 'yellow' in file_name:\n",
    "            file_type = 'yellow'\n",
    "        elif 'green' in file_name:\n",
    "            file_type = 'green'\n",
    "        elif 'hvfht' in file_name:\n",
    "            file_type = 'hvfht'\n",
    "        else:\n",
    "            logging.error(f\"Unknown file type for file: {file_name}\")\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_parquet(file_path)\n",
    "        df, daily_agg = clean_and_transform(df, file_type)\n",
    "        if df is not None:\n",
    "            processed_data.append(df)\n",
    "        if daily_agg is not None:\n",
    "            daily_aggregates.append(daily_agg)\n",
    "    \n",
    "    if processed_data:\n",
    "        processed_data = pd.concat(processed_data)\n",
    "    else:\n",
    "        processed_data = pd.DataFrame()\n",
    "    \n",
    "    if daily_aggregates:\n",
    "        daily_aggregates = pd.concat(daily_aggregates)\n",
    "    else:\n",
    "        daily_aggregates = pd.DataFrame()\n",
    "    \n",
    "    return processed_data, daily_aggregates\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/tmp/nyc_taxi_data_2019\"\n",
    "    processed_data, daily_aggregates = process_all_files(data_folder)\n",
    "    if not processed_data.empty:\n",
    "        processed_data.to_parquet(\"/dbfs/tmp/processed_data.parquet\", index=False)\n",
    "    if not daily_aggregates.empty:\n",
    "        daily_aggregates.to_parquet(\"/dbfs/tmp/daily_aggregates.parquet\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bf68200-d6fa-4250-ac61-9d26cb2fcc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5317ba35-284d-4591-ae88-4d6bd70edabc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2750058774820529>:53\u001B[0m\n",
       "\u001B[1;32m     50\u001B[0m all_processed_data \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m     51\u001B[0m all_daily_aggregates \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m local_file \u001B[38;5;129;01min\u001B[39;00m parquet_files:\n",
       "\u001B[1;32m     54\u001B[0m     dbfs_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dbfs_dir, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(local_file))\n",
       "\u001B[1;32m     55\u001B[0m     taxi_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'parquet_files' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2750058774820529>:53\u001B[0m\n\u001B[1;32m     50\u001B[0m all_processed_data \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     51\u001B[0m all_daily_aggregates \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m local_file \u001B[38;5;129;01min\u001B[39;00m parquet_files:\n\u001B[1;32m     54\u001B[0m     dbfs_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dbfs_dir, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(local_file))\n\u001B[1;32m     55\u001B[0m     taxi_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\n\u001B[0;31mNameError\u001B[0m: name 'parquet_files' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'parquet_files' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to clean, transform, and aggregate data for Spark DataFrame\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def clean_transform_aggregate_spark(df, taxi_type):\n",
    "    if taxi_type == \"yellow\":\n",
    "        required_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df = df.withColumn('pickup_datetime', F.to_timestamp('tpep_pickup_datetime')) \\\n",
    "               .withColumn('dropoff_datetime', F.to_timestamp('tpep_dropoff_datetime'))\n",
    "    elif taxi_type == \"green\":\n",
    "        required_columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df = df.withColumn('pickup_datetime', F.to_timestamp('lpep_pickup_datetime')) \\\n",
    "               .withColumn('dropoff_datetime', F.to_timestamp('lpep_dropoff_datetime'))\n",
    "    elif taxi_type == \"hvfht\":\n",
    "        required_columns = ['pickup_datetime', 'dropoff_datetime', 'base_passenger_fare']\n",
    "        if 'trip_miles' in df.columns:\n",
    "            required_columns.append('trip_miles')\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df = df.withColumn('pickup_datetime', F.to_timestamp('pickup_datetime')) \\\n",
    "               .withColumn('dropoff_datetime', F.to_timestamp('dropoff_datetime'))\n",
    "    \n",
    "    if 'trip_miles' in df.columns:\n",
    "        df = df.withColumn('trip_duration', (F.col('dropoff_datetime').cast('long') - F.col('pickup_datetime').cast('long')) / 60) \\\n",
    "               .withColumn('average_speed', F.col('trip_miles') / (F.col('trip_duration') / 60))\n",
    "    elif 'trip_distance' in df.columns:\n",
    "        df = df.withColumn('trip_duration', (F.col('dropoff_datetime').cast('long') - F.col('pickup_datetime').cast('long')) / 60) \\\n",
    "               .withColumn('average_speed', F.col('trip_distance') / (F.col('trip_duration') / 60))\n",
    "    \n",
    "    df = df.withColumn('date', F.to_date('pickup_datetime'))\n",
    "    \n",
    "    # Aggregate data to calculate total trips and average fare per day\n",
    "    if 'fare_amount' in df.columns:\n",
    "        daily_aggregates = df.groupBy('date').agg(\n",
    "            F.count('*').alias('total_trips'),\n",
    "            F.mean('fare_amount').alias('average_fare')\n",
    "        )\n",
    "    elif 'base_passenger_fare' in df.columns:\n",
    "        daily_aggregates = df.groupBy('date').agg(\n",
    "            F.count('*').alias('total_trips'),\n",
    "            F.mean('base_passenger_fare').alias('average_fare')\n",
    "        )\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "# Read, clean, transform, aggregate, and display data from each parquet file in DBFS using Spark\n",
    "all_processed_data = []\n",
    "all_daily_aggregates = []\n",
    "\n",
    "for local_file in parquet_files:\n",
    "    dbfs_file = os.path.join(dbfs_dir, os.path.basename(local_file))\n",
    "    taxi_type = \"\"\n",
    "    if \"yellow\" in dbfs_file:\n",
    "        taxi_type = \"yellow\"\n",
    "    elif \"green\" in dbfs_file:\n",
    "        taxi_type = \"green\"\n",
    "    elif \"hvfht\" in dbfs_file:\n",
    "        taxi_type = \"hvfht\"\n",
    "    \n",
    "    if taxi_type:\n",
    "        df = spark.read.parquet(dbfs_file)\n",
    "        print(f\"Schema for {taxi_type} taxi data:\")\n",
    "        df.printSchema()  # Print the schema for debugging purposes\n",
    "        processed_df, daily_agg_df = clean_transform_aggregate_spark(df, taxi_type)\n",
    "        \n",
    "        processed_df.show(5)\n",
    "        daily_agg_df.show(5)\n",
    "        \n",
    "        all_processed_data.append(processed_df)\n",
    "        all_daily_aggregates.append(daily_agg_df)\n",
    "\n",
    "# Concatenate all processed data and daily aggregates\n",
    "final_processed_data = all_processed_data[0]\n",
    "final_daily_aggregates = all_daily_aggregates[0]\n",
    "\n",
    "for df in all_processed_data[1:]:\n",
    "    final_processed_data = final_processed_data.union(df)\n",
    "\n",
    "for df in all_daily_aggregates[1:]:\n",
    "    final_daily_aggregates = final_daily_aggregates.union(df)\n",
    "\n",
    "# Save the final processed data and daily aggregates to parquet files\n",
    "final_processed_data.write.parquet(\"/dbfs/tmp/processed_data.parquet\", mode='overwrite')\n",
    "final_daily_aggregates.write.parquet(\"/dbfs/tmp/daily_aggregates.parquet\", mode='overwrite')\n",
    "\n",
    "print(\"Data loaded, cleaned, transformed, aggregated, and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af7d2d3c-a00d-4e42-b613-ea8005e3e212",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2750058774820525>:5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m all_processed_data \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m      3\u001B[0m all_daily_aggregates \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m local_file \u001B[38;5;129;01min\u001B[39;00m parquet_files:\n",
       "\u001B[1;32m      6\u001B[0m     dbfs_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dbfs_dir, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(local_file))\n",
       "\u001B[1;32m      7\u001B[0m     taxi_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'parquet_files' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2750058774820525>:5\u001B[0m\n\u001B[1;32m      2\u001B[0m all_processed_data \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m all_daily_aggregates \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m local_file \u001B[38;5;129;01min\u001B[39;00m parquet_files:\n\u001B[1;32m      6\u001B[0m     dbfs_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dbfs_dir, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(local_file))\n\u001B[1;32m      7\u001B[0m     taxi_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\n\u001B[0;31mNameError\u001B[0m: name 'parquet_files' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'parquet_files' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read, clean, transform, aggregate, and display data from each parquet file in DBFS using Spark\n",
    "all_processed_data = []\n",
    "all_daily_aggregates = []\n",
    "\n",
    "for local_file in parquet_files:\n",
    "    dbfs_file = os.path.join(dbfs_dir, os.path.basename(local_file))\n",
    "    taxi_type = \"\"\n",
    "    if \"yellow\" in dbfs_file:\n",
    "        taxi_type = \"yellow\"\n",
    "    elif \"green\" in dbfs_file:\n",
    "        taxi_type = \"green\"\n",
    "    elif \"fhv\" in dbfs_file:\n",
    "        taxi_type = \"fhv\"\n",
    "    elif \"hvfht\" in dbfs_file:\n",
    "        taxi_type = \"hvfht\"\n",
    "\n",
    "if taxi_type:\n",
    "        df = spark.read.parquet(dbfs_file)\n",
    "        print(f\"Schema for {taxi_type} taxi data:\")\n",
    "        df.printSchema()  # Print the schema for debugging purposes\n",
    "        processed_df, daily_agg_df = clean_transform_aggregate_spark(df, taxi_type)\n",
    "        \n",
    "        processed_df.show(5)\n",
    "        daily_agg_df.show(5)\n",
    "        \n",
    "        all_processed_data.append(processed_df)\n",
    "        all_daily_aggregates.append(daily_agg_df)\n",
    "\n",
    "# Concatenate all processed data and daily aggregates\n",
    "final_processed_data = all_processed_data[0]\n",
    "final_daily_aggregates = all_daily_aggregates[0]\n",
    "\n",
    "for df in all_processed_data[1:]:\n",
    "    final_processed_data = final_processed_data.union(df)\n",
    "\n",
    "for df in all_daily_aggregates[1:]:\n",
    "    final_daily_aggregates = final_daily_aggregates.union(df)\n",
    "\n",
    "# Save the final processed data and daily aggregates to parquet files\n",
    "final_processed_data.write.parquet(\"/dbfs/tmp/processed_data.parquet\", mode='overwrite')\n",
    "final_daily_aggregates.write.parquet(\"/dbfs/tmp/daily_aggregates.parquet\", mode='overwrite')\n",
    "\n",
    "print(\"Data loaded, cleaned, transformed, aggregated, and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08998de3-e735-48f2-8ed8-466ff501f059",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2436500203063026>:5\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m all_processed_data \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[1;32m      3\u001B[0m all_daily_aggregates \u001B[38;5;241m=\u001B[39m []\n",
       "\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m local_file \u001B[38;5;129;01min\u001B[39;00m parquet_files:\n",
       "\u001B[1;32m      6\u001B[0m     dbfs_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m dbfs_dir, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(local_file))\n",
       "\u001B[1;32m      7\u001B[0m     taxi_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'parquet_files' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-2436500203063026>:5\u001B[0m\n\u001B[1;32m      2\u001B[0m all_processed_data \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      3\u001B[0m all_daily_aggregates \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m local_file \u001B[38;5;129;01min\u001B[39;00m parquet_files:\n\u001B[1;32m      6\u001B[0m     dbfs_file \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m dbfs_dir, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(local_file))\n\u001B[1;32m      7\u001B[0m     taxi_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\n\u001B[0;31mNameError\u001B[0m: name 'parquet_files' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'parquet_files' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read, clean, transform, aggregate, and display data from each parquet file in DBFS using Spark\n",
    "all_processed_data = []\n",
    "all_daily_aggregates = []\n",
    "\n",
    "for local_file in parquet_files:\n",
    "    dbfs_file = os.path.join(\"/dbfs\" + dbfs_dir, os.path.basename(local_file))\n",
    "    taxi_type = \"\"\n",
    "    if \"yellow\" in dbfs_file:\n",
    "        taxi_type = \"yellow\"\n",
    "    elif \"green\" in dbfs_file:\n",
    "        taxi_type = \"green\"\n",
    "    elif \"fhv\" in dbfs_file:\n",
    "        taxi_type = \"fhv\"\n",
    "    elif \"hvfht\" in dbfs_file:\n",
    "        taxi_type = \"hvfht\"\n",
    "    \n",
    "    if taxi_type:\n",
    "        df = spark.read.parquet(dbfs_file)\n",
    "        print(f\"Schema for {taxi_type} taxi data:\")\n",
    "        df.printSchema()  # Print the schema for debugging purposes\n",
    "        processed_df, daily_agg_df = clean_transform_aggregate_spark(df, taxi_type)\n",
    "        \n",
    "        processed_df.show(5)\n",
    "        daily_agg_df.show(5)\n",
    "        \n",
    "        all_processed_data.append(processed_df)\n",
    "        all_daily_aggregates.append(daily_agg_df)\n",
    "\n",
    "# Concatenate all processed data and daily aggregates\n",
    "final_processed_data = all_processed_data[0]\n",
    "final_daily_aggregates = all_daily_aggregates[0]\n",
    "\n",
    "for df in all_processed_data[1:]:\n",
    "    final_processed_data = final_processed_data.union(df)\n",
    "\n",
    "for df in all_daily_aggregates[1:]:\n",
    "    final_daily_aggregates = final_daily_aggregates.union(df)\n",
    "\n",
    "# Save the final processed data and daily aggregates to parquet files\n",
    "final_processed_data.write.parquet(\"/dbfs/tmp/processed_data.parquet\", mode='overwrite')\n",
    "final_daily_aggregates.write.parquet(\"/dbfs/tmp/daily_aggregates.parquet\", mode='overwrite')\n",
    "\n",
    "print(\"Data loaded, cleaned, transformed, aggregated, and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "926f9e78-f6ee-4efa-a4df-988580f8bb88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "elif taxi_type == \"fhvhv\":\n",
    "        required_columns = ['pickup_datetime', 'dropoff_datetime', 'base_passenger_fare']\n",
    "        if 'trip_miles' in df.columns:\n",
    "            required_columns.append('trip_miles')\n",
    "        df = df.dropna(subset=required_columns)\n",
    "        df = df.withColumn('pickup_datetime', F.to_timestamp('pickup_datetime')) \\\n",
    "               .withColumn('dropoff_datetime', F.to_timestamp('dropoff_datetime'))\n",
    "\n",
    "\n",
    "               elif \"fhv\" in dbfs_file:\n",
    "        taxi_type = \"fhv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b810741-d44c-4c93-b8c0-d87979826fbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_transform_yellow(df):\n",
    "    required_columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise KeyError(f\"Missing required columns in yellow taxi data: {required_columns}\")\n",
    "    \n",
    "    df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def clean_and_transform_green(df):\n",
    "    required_columns = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'trip_distance', 'fare_amount']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise KeyError(f\"Missing required columns in green taxi data: {required_columns}\")\n",
    "\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def clean_and_transform_hvfht(df):\n",
    "    required_columns = ['pickup_datetime', 'dropoff_datetime', 'trip_miles', 'base_passenger_fare']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        raise KeyError(f\"Missing required columns in HVFHT data: {required_columns}\")\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_miles'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'base_passenger_fare': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'base_passenger_fare': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def process_all_files(data_folder):\n",
    "    processed_data = []\n",
    "    daily_aggregates = []\n",
    "    \n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        try:\n",
    "            if 'yellow' in file_name:\n",
    "                df, daily_agg = clean_and_transform_yellow(df)\n",
    "            elif 'green' in file_name:\n",
    "                df, daily_agg = clean_and_transform_green(df)\n",
    "            elif 'hvfht' in file_name:\n",
    "                df, daily_agg = clean_and_transform_hvfht(df)\n",
    "            else:\n",
    "                continue  # skip files that do not match any known type\n",
    "            \n",
    "            processed_data.append(df)\n",
    "            daily_aggregates.append(daily_agg)\n",
    "        \n",
    "        except KeyError as e:\n",
    "            print(f\"Skipping file {file_name} due to missing columns: {e}\")\n",
    "    \n",
    "    return pd.concat(processed_data), pd.concat(daily_aggregates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/tmp/nyc_taxi_data_2019\"\n",
    "    processed_data, daily_aggregates = process_all_files(data_folder)\n",
    "    processed_data.to_parquet(\"/dbfs/tmp/processed_data.parquet\", index=False)\n",
    "    daily_aggregates.to_parquet(\"/dbfs/tmp/daily_aggregates.parquet\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0275c413-df61-47e6-8c24-5263a42679bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample code to inspect the schema\n",
    "data_folder = \"/mnt/nyc_taxi_data_2019\"\n",
    "file_path = f\"{data_folder}/yellow_tripdata_2019-01.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5de764-82cf-4778-b05b-b2d8882677d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample code to inspect the schema\n",
    "data_folder = \"/mnt/nyc_taxi_data_2019\"\n",
    "file_path = f\"{data_folder}/green_tripdata_2019-01.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a805165-5c2e-43a7-ae04-f91b251c2c55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample code to inspect the schema\n",
    "data_folder = \"/mnt/nyc_taxi_data_2019\"\n",
    "file_path = f\"{data_folder}/fhv_tripdata_2019-01.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1baa6200-e382-475f-95f7-0ada9077ac0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_folder = \"/mnt/nyc_taxi_data_2019\"\n",
    "file_path = f\"{data_folder}/fhvhv_tripdata_2019-02.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9940b928-dac7-4839-8d61-25dd26e5eaa9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample code to inspect the schema\n",
    "data_folder = \"/mnt/nyc_taxi_data_2019\"\n",
    "file_path = f\"{data_folder}/fhvhv_tripdata_2019-02.parquet\"\n",
    "df = spark.read.parquet(file_path)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d13a42e-50c2-4f0f-ba0f-ff93f5bb8fec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70ab3ea-5f2e-40bb-bb32-2ad0ad75e94e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3462543825505870>:83\u001B[0m\n",
       "\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m     82\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 83\u001B[0m     processed_data, daily_aggregates \u001B[38;5;241m=\u001B[39m process_all_files(data_folder)\n",
       "\u001B[1;32m     84\u001B[0m     processed_data\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/processed_data.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m     85\u001B[0m     daily_aggregates\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/daily_aggregates.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m<command-3462543825505870>:74\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n",
       "\u001B[1;32m     72\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m clean_and_transform_green(df)\n",
       "\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m---> 74\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m \u001B[43mclean_and_transform_fhvht\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     76\u001B[0m processed_data\u001B[38;5;241m.\u001B[39mappend(df)\n",
       "\u001B[1;32m     77\u001B[0m daily_aggregates\u001B[38;5;241m.\u001B[39mappend(daily_agg)\n",
       "\n",
       "File \u001B[0;32m<command-3462543825505870>:48\u001B[0m, in \u001B[0;36mclean_and_transform_fhvht\u001B[0;34m(df)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Derive new columns\u001B[39;00m\n",
       "\u001B[1;32m     47\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[0;32m---> 48\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdropoff_datetime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n",
       "\u001B[1;32m     49\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\n",
       "\u001B[1;32m     50\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maverage_speed\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_miles\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n",
       "\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n",
       "\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n",
       "\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
       "\u001B[1;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
       "\u001B[1;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n",
       "\u001B[1;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n",
       "\u001B[1;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
       "\u001B[1;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\n\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'\n\nThe above exception was the direct cause of the following exception:\n\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-3462543825505870>:83\u001B[0m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     82\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 83\u001B[0m     processed_data, daily_aggregates \u001B[38;5;241m=\u001B[39m process_all_files(data_folder)\n\u001B[1;32m     84\u001B[0m     processed_data\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/processed_data.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     85\u001B[0m     daily_aggregates\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/daily_aggregates.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\nFile \u001B[0;32m<command-3462543825505870>:74\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n\u001B[1;32m     72\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m clean_and_transform_green(df)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 74\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m \u001B[43mclean_and_transform_fhvht\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     76\u001B[0m processed_data\u001B[38;5;241m.\u001B[39mappend(df)\n\u001B[1;32m     77\u001B[0m daily_aggregates\u001B[38;5;241m.\u001B[39mappend(daily_agg)\n\nFile \u001B[0;32m<command-3462543825505870>:48\u001B[0m, in \u001B[0;36mclean_and_transform_fhvht\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Derive new columns\u001B[39;00m\n\u001B[1;32m     47\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 48\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdropoff_datetime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     49\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\n\u001B[1;32m     50\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maverage_speed\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_miles\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m)\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\n\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'dropoff_datetime'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_transform_yellow(df):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def clean_and_transform_green(df):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def clean_and_transform_fhvht(df):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_miles'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'base_passenger_fare': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'base_passenger_fare': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def process_all_files(data_folder):\n",
    "    processed_data = []\n",
    "    daily_aggregates = []\n",
    "    \n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        if 'yellow' in file_name:\n",
    "            df, daily_agg = clean_and_transform_yellow(df)\n",
    "        elif 'green' in file_name:\n",
    "            df, daily_agg = clean_and_transform_green(df)\n",
    "        else:\n",
    "            df, daily_agg = clean_and_transform_fhvht(df)\n",
    "        \n",
    "        processed_data.append(df)\n",
    "        daily_aggregates.append(daily_agg)\n",
    "    \n",
    "    return pd.concat(processed_data), pd.concat(daily_aggregates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/tmp/nyc_taxi_data_2019\"\n",
    "    processed_data, daily_aggregates = process_all_files(data_folder)\n",
    "    processed_data.to_parquet(\"/dbfs/tmp/processed_data.parquet\", index=False)\n",
    "    daily_aggregates.to_parquet(\"/dbfs/tmp/daily_aggregates.parquet\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fae0703-694f-48af-ab0f-4751ed71ce09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c1bed3-c160-40f0-9e0a-de4b4390f0e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3462543825505872>:106\u001B[0m\n",
       "\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    105\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 106\u001B[0m     processed_data, daily_aggregates \u001B[38;5;241m=\u001B[39m process_all_files(data_folder)\n",
       "\u001B[1;32m    107\u001B[0m     processed_data\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/processed_data.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m    108\u001B[0m     daily_aggregates\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/daily_aggregates.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m<command-3462543825505872>:93\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n",
       "\u001B[1;32m     91\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m clean_and_transform_green(df)\n",
       "\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfhv\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m file_name:\n",
       "\u001B[0;32m---> 93\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m \u001B[43mclean_and_transform_fhv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhvfht\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m file_name:\n",
       "\u001B[1;32m     95\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m clean_and_transform_hvfht(df)\n",
       "\n",
       "File \u001B[0;32m<command-3462543825505872>:48\u001B[0m, in \u001B[0;36mclean_and_transform_fhv\u001B[0;34m(df)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Derive new columns\u001B[39;00m\n",
       "\u001B[1;32m     47\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[0;32m---> 48\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdropoff_datetime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n",
       "\u001B[1;32m     49\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\n",
       "\u001B[1;32m     50\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maverage_speed\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_miles\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n",
       "\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n",
       "\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n",
       "\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
       "\u001B[1;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
       "\u001B[1;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n",
       "\u001B[1;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n",
       "\u001B[1;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
       "\u001B[1;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\n\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'\n\nThe above exception was the direct cause of the following exception:\n\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-3462543825505872>:106\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    105\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 106\u001B[0m     processed_data, daily_aggregates \u001B[38;5;241m=\u001B[39m process_all_files(data_folder)\n\u001B[1;32m    107\u001B[0m     processed_data\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/processed_data.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    108\u001B[0m     daily_aggregates\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/daily_aggregates.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\nFile \u001B[0;32m<command-3462543825505872>:93\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n\u001B[1;32m     91\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m clean_and_transform_green(df)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfhv\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m file_name:\n\u001B[0;32m---> 93\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m \u001B[43mclean_and_transform_fhv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhvfht\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m file_name:\n\u001B[1;32m     95\u001B[0m     df, daily_agg \u001B[38;5;241m=\u001B[39m clean_and_transform_hvfht(df)\n\nFile \u001B[0;32m<command-3462543825505872>:48\u001B[0m, in \u001B[0;36mclean_and_transform_fhv\u001B[0;34m(df)\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Derive new columns\u001B[39;00m\n\u001B[1;32m     47\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 48\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdropoff_datetime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     49\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\n\u001B[1;32m     50\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maverage_speed\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_miles\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m)\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\n\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'dropoff_datetime'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_transform_yellow(df):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def clean_and_transform_green(df):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def clean_and_transform_fhv(df):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_miles'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'base_passenger_fare': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'base_passenger_fare': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def clean_and_transform_hvfht(df):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_miles'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'base_passenger_fare': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'base_passenger_fare': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def process_all_files(data_folder):\n",
    "    processed_data = []\n",
    "    daily_aggregates = []\n",
    "    \n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        if 'yellow' in file_name:\n",
    "            df, daily_agg = clean_and_transform_yellow(df)\n",
    "        elif 'green' in file_name:\n",
    "            df, daily_agg = clean_and_transform_green(df)\n",
    "        elif 'fhv' in file_name:\n",
    "            df, daily_agg = clean_and_transform_fhv(df)\n",
    "        elif 'hvfht' in file_name:\n",
    "            df, daily_agg = clean_and_transform_hvfht(df)\n",
    "        else:\n",
    "            continue  # skip files that do not match any known type\n",
    "        \n",
    "        processed_data.append(df)\n",
    "        daily_aggregates.append(daily_agg)\n",
    "    \n",
    "    return pd.concat(processed_data), pd.concat(daily_aggregates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/tmp/nyc_taxi_data_2019\"\n",
    "    processed_data, daily_aggregates = process_all_files(data_folder)\n",
    "    processed_data.to_parquet(\"/dbfs/tmp/processed_data.parquet\", index=False)\n",
    "    daily_aggregates.to_parquet(\"/dbfs/tmp/daily_aggregates.parquet\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4781bd75-07af-47bc-9946-919895a57a1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-783755515913074>:54\u001B[0m\n",
       "\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m     53\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 54\u001B[0m     processed_data, daily_aggregates \u001B[38;5;241m=\u001B[39m process_all_files(data_folder)\n",
       "\u001B[1;32m     55\u001B[0m     processed_data\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/processed_data.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m     56\u001B[0m     daily_aggregates\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/daily_aggregates.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m<command-783755515913074>:46\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n",
       "\u001B[1;32m     44\u001B[0m file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_folder, file_name)\n",
       "\u001B[1;32m     45\u001B[0m taxi_type \u001B[38;5;241m=\u001B[39m file_name\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n",
       "\u001B[0;32m---> 46\u001B[0m df, daily_agg \u001B[38;5;241m=\u001B[39m \u001B[43mclean_and_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtaxi_type\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     47\u001B[0m processed_data\u001B[38;5;241m.\u001B[39mappend(df)\n",
       "\u001B[1;32m     48\u001B[0m daily_aggregates\u001B[38;5;241m.\u001B[39mappend(daily_agg)\n",
       "\n",
       "File \u001B[0;32m<command-783755515913074>:26\u001B[0m, in \u001B[0;36mclean_and_transform\u001B[0;34m(file_path, taxi_type)\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m taxi_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfhv\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhvfht\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n",
       "\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# Columns specific to FHV and HVFHT taxi\u001B[39;00m\n",
       "\u001B[1;32m     25\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[0;32m---> 26\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdropoff_datetime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n",
       "\u001B[1;32m     27\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\n",
       "\u001B[1;32m     28\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maverage_speed\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_miles\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n",
       "\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n",
       "\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n",
       "\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n",
       "\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n",
       "\u001B[1;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n",
       "\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n",
       "\u001B[1;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n",
       "\u001B[1;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n",
       "\u001B[1;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n",
       "\u001B[1;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n",
       "\u001B[1;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
       "\n",
       "\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\nFile \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n\n\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'\n\nThe above exception was the direct cause of the following exception:\n\n\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)\nFile \u001B[0;32m<command-783755515913074>:54\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     53\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 54\u001B[0m     processed_data, daily_aggregates \u001B[38;5;241m=\u001B[39m process_all_files(data_folder)\n\u001B[1;32m     55\u001B[0m     processed_data\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/processed_data.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m     56\u001B[0m     daily_aggregates\u001B[38;5;241m.\u001B[39mto_parquet(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/dbfs/tmp/daily_aggregates.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\nFile \u001B[0;32m<command-783755515913074>:46\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n\u001B[1;32m     44\u001B[0m file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_folder, file_name)\n\u001B[1;32m     45\u001B[0m taxi_type \u001B[38;5;241m=\u001B[39m file_name\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m---> 46\u001B[0m df, daily_agg \u001B[38;5;241m=\u001B[39m \u001B[43mclean_and_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtaxi_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m processed_data\u001B[38;5;241m.\u001B[39mappend(df)\n\u001B[1;32m     48\u001B[0m daily_aggregates\u001B[38;5;241m.\u001B[39mappend(daily_agg)\n\nFile \u001B[0;32m<command-783755515913074>:26\u001B[0m, in \u001B[0;36mclean_and_transform\u001B[0;34m(file_path, taxi_type)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m taxi_type \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfhv\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhvfht\u001B[39m\u001B[38;5;124m'\u001B[39m]:\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;66;03m# Columns specific to FHV and HVFHT taxi\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 26\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdropoff_datetime\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m)\n\u001B[1;32m     27\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mtotal_seconds() \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m\n\u001B[1;32m     28\u001B[0m     df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maverage_speed\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_miles\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m (df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrip_duration\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m60\u001B[39m)\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n\n\u001B[0;31mKeyError\u001B[0m: 'dropoff_datetime'",
       "errorSummary": "<span class='ansi-red-fg'>KeyError</span>: 'dropoff_datetime'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_transform(file_path, taxi_type):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Schema-specific processing\n",
    "    if taxi_type in ['yellow']:\n",
    "        # Columns specific to yellow taxi\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "        df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "        df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    elif taxi_type in [ 'green']:\n",
    "        # Columns specific to green taxi\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "        df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "        df['average_speed'] = df['trip_miles'] / (df['trip_duration'] / 60)\n",
    "    elif taxi_type in ['fhv', 'hvfht']:\n",
    "        # Columns specific to FHV and HVFHT taxi\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "        df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "        df['average_speed'] = df['trip_miles'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean' if taxi_type in ['yellow', 'green'] else 'base_passenger_fare'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare' if taxi_type in ['yellow', 'green'] else 'base_passenger_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def process_all_files(data_folder):\n",
    "    processed_data = []\n",
    "    daily_aggregates = []\n",
    "    \n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        taxi_type = file_name.split('_')[0]\n",
    "        df, daily_agg = clean_and_transform(file_path, taxi_type)\n",
    "        processed_data.append(df)\n",
    "        daily_aggregates.append(daily_agg)\n",
    "    \n",
    "    return pd.concat(processed_data), pd.concat(daily_aggregates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/tmp/nyc_taxi_data_2019\"\n",
    "    processed_data, daily_aggregates = process_all_files(data_folder)\n",
    "    processed_data.to_parquet(\"/dbfs/tmp/processed_data.parquet\", index=False)\n",
    "    daily_aggregates.to_parquet(\"/dbfs/tmp/daily_aggregates.parquet\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcebc6e3-2f80-4c15-8bf0-1abd7745d74c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove trips with missing or corrupt data\n",
    "# Assuming 'pickup_datetime', 'dropoff_datetime', 'trip_distance', and 'fare_amount' are essential columns\n",
    "df = df.dropna(subset=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66538b4-7dd3-4c69-aae5-1b0487809a0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-783755515913070>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Convert datetime columns to datetime objects\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_pickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_pickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\u001B[1;32m      3\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_dropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_dropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:1046\u001B[0m, in \u001B[0;36mto_datetime\u001B[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001B[0m\n",
       "\u001B[1;32m   1044\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1045\u001B[0m             result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39mtz_localize(tz)\n",
       "\u001B[0;32m-> 1046\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mABCSeries\u001B[49m\u001B[43m)\u001B[49m:\n",
       "\u001B[1;32m   1047\u001B[0m     cache_array \u001B[38;5;241m=\u001B[39m _maybe_cache(arg, \u001B[38;5;28mformat\u001B[39m, cache, convert_listlike)\n",
       "\u001B[1;32m   1048\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m cache_array\u001B[38;5;241m.\u001B[39mempty:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/dtypes/generic.py:45\u001B[0m, in \u001B[0;36mcreate_pandas_abc_type.<locals>._check\u001B[0;34m(cls, inst)\u001B[0m\n",
       "\u001B[1;32m     43\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n",
       "\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check\u001B[39m(\u001B[38;5;28mcls\u001B[39m, inst) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
       "\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_typ\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcomp\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:1295\u001B[0m, in \u001B[0;36mColumn.__nonzero__\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__nonzero__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m   1296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot convert column into bool: please use \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m&\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mor\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1297\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m~\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m when building DataFrame boolean expressions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   1298\u001B[0m     )\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-783755515913070>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Convert datetime columns to datetime objects\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_pickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_pickup_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      3\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_dropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mto_datetime(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtpep_dropoff_datetime\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/tools/datetimes.py:1046\u001B[0m, in \u001B[0;36mto_datetime\u001B[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001B[0m\n\u001B[1;32m   1044\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1045\u001B[0m             result \u001B[38;5;241m=\u001B[39m arg\u001B[38;5;241m.\u001B[39mtz_localize(tz)\n\u001B[0;32m-> 1046\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mABCSeries\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1047\u001B[0m     cache_array \u001B[38;5;241m=\u001B[39m _maybe_cache(arg, \u001B[38;5;28mformat\u001B[39m, cache, convert_listlike)\n\u001B[1;32m   1048\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m cache_array\u001B[38;5;241m.\u001B[39mempty:\n\nFile \u001B[0;32m/databricks/python/lib/python3.9/site-packages/pandas/core/dtypes/generic.py:45\u001B[0m, in \u001B[0;36mcreate_pandas_abc_type.<locals>._check\u001B[0;34m(cls, inst)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check\u001B[39m(\u001B[38;5;28mcls\u001B[39m, inst) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m---> 45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m_typ\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcomp\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/column.py:1295\u001B[0m, in \u001B[0;36mColumn.__nonzero__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1294\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__nonzero__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1296\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot convert column into bool: please use \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m&\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mand\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m|\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mor\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1297\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m~\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m for \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m when building DataFrame boolean expressions.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1298\u001B[0m     )\n\n\u001B[0;31mValueError\u001B[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert datetime columns to datetime objects\n",
    "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "845e6eda-12b1-452b-ac53-41b4eaa89cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Derive new columns: trip duration in minutes and average speed in miles per hour\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)  # converting duration to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a5d7505-924c-405d-b87c-8fa98ea5c948",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate data to calculate total trips and average fare per day\n",
    "df['day'] = df['pickup_datetime'].dt.date\n",
    "aggregated_data = df.groupby('day').agg(\n",
    "    total_trips=('day', 'size'),\n",
    "    average_fare=('fare_amount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Display the aggregated data\n",
    "print(aggregated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d475285-d8b9-47e6-826d-fb78b73edc77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49f45735-b065-48de-9d64-dc1a4ab8aa4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = '/tmp/nyc_taxi_data_2019'\n",
    "file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "print(f\"File size: {file_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fedd3a-427c-450e-bb1a-2b77e0c190dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4939ede-6a8b-4d2b-9efc-0f7ac4c11264",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'%pip --disable-pip-version-check install dask dataframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e68f74-a9eb-4f6b-a576-88288a17bb02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/__init__.py:52\u001B[0m\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compute\n",
       "\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backends, dispatch, methods, rolling\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_testing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m test_dataframe\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/backends.py:14\u001B[0m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CreationDispatch, DaskBackendEntrypoint\n",
       "\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PANDAS_GE_220, is_any_real_numeric_dtype\n",
       "\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataFrame, Index, Scalar, Series, _Frame\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/_compat.py:9\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compatibility\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m import_optional_dependency\n",
       "\u001B[0;32m----> 9\u001B[0m \u001B[43mimport_optional_dependency\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpandas\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     10\u001B[0m import_optional_dependency(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/_compatibility.py:135\u001B[0m, in \u001B[0;36mimport_optional_dependency\u001B[0;34m(name, extra, min_version, errors)\u001B[0m\n",
       "\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n",
       "\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mImportError\u001B[0m: Dask requires version '2.0.0' or newer of 'pandas' (version '1.4.2' currently installed).\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3173150108128263>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdd\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the Parquet file using Dask\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m df \u001B[38;5;241m=\u001B[39m dd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/__init__.py:122\u001B[0m\n",
       "\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    116\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n",
       "\u001B[1;32m    117\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDask dataframe requirements are not installed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    118\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease either conda or pip install as follows:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    119\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  conda install dask                     # either conda install\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    120\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m  python -m pip install \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdask[dataframe]\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m --upgrade  # or python -m pip install\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m    121\u001B[0m     )\n",
       "\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _dask_expr_enabled():\n",
       "\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mImportError\u001B[0m: Dask dataframe requirements are not installed.\n",
       "\n",
       "Please either conda or pip install as follows:\n",
       "\n",
       "  conda install dask                     # either conda install\n",
       "  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/__init__.py:52\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compute\n\u001B[0;32m---> 52\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backends, dispatch, methods, rolling\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_testing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m test_dataframe\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/backends.py:14\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CreationDispatch, DaskBackendEntrypoint\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PANDAS_GE_220, is_any_real_numeric_dtype\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataFrame, Index, Scalar, Series, _Frame\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/_compat.py:9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_compatibility\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m import_optional_dependency\n\u001B[0;32m----> 9\u001B[0m \u001B[43mimport_optional_dependency\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpandas\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m import_optional_dependency(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/_compatibility.py:135\u001B[0m, in \u001B[0;36mimport_optional_dependency\u001B[0;34m(name, extra, min_version, errors)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg)\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\n\u001B[0;31mImportError\u001B[0m: Dask requires version '2.0.0' or newer of 'pandas' (version '1.4.2' currently installed).\n\nThe above exception was the direct cause of the following exception:\n\n\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)\nFile \u001B[0;32m<command-3173150108128263>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdask\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataframe\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mdd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the Parquet file using Dask\u001B[39;00m\n\u001B[1;32m      4\u001B[0m df \u001B[38;5;241m=\u001B[39m dd\u001B[38;5;241m.\u001B[39mread_parquet(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-26985d3b-357a-4fcb-bef6-2c0da7e59a90/lib/python3.9/site-packages/dask/dataframe/__init__.py:122\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    116\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    117\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDask dataframe requirements are not installed.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    118\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease either conda or pip install as follows:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    119\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  conda install dask                     # either conda install\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m  python -m pip install \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdask[dataframe]\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m --upgrade  # or python -m pip install\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    121\u001B[0m     )\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _dask_expr_enabled():\n\u001B[1;32m    126\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\n\u001B[0;31mImportError\u001B[0m: Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install",
       "errorSummary": "<span class='ansi-red-fg'>ImportError</span>: Dask dataframe requirements are not installed.\n\nPlease either conda or pip install as follows:\n\n  conda install dask                     # either conda install\n  python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the Parquet file using Dask\n",
    "df = dd.read_parquet('/tmp/nyc_taxi_data_2019')\n",
    "\n",
    "# Remove trips with missing or corrupt data\n",
    "df = df.dropna(subset=['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'fare_amount'])\n",
    "\n",
    "# Convert datetime columns to datetime objects\n",
    "df['pickup_datetime'] = dd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = dd.to_datetime(df['dropoff_datetime'])\n",
    "\n",
    "# Derive new columns: trip duration in minutes and average speed in miles per hour\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)  # converting duration to hours\n",
    "\n",
    "# Aggregate data to calculate total trips and average fare per day\n",
    "df['day'] = df['pickup_datetime'].dt.date\n",
    "aggregated_data = df.groupby('day').agg(\n",
    "    total_trips=('day', 'size'),\n",
    "    average_fare=('fare_amount', 'mean')\n",
    ").compute().reset_index()\n",
    "\n",
    "# Display the aggregated data\n",
    "print(aggregated_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f8ad21-51d7-473b-b1c1-ec0916ebd9a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stderr:\u001B[0m\n",
       "\u001B[0;31mThu Jul 11 07:05:53 2024 Connection to spark from PID  1083\u001B[0m\n",
       "\u001B[0;31mThu Jul 11 07:05:53 2024 Initialized gateway on port 40959\u001B[0m\n",
       "\u001B[0;31mThu Jul 11 07:05:54 2024 Connected to spark.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stdout:\u001B[0m\n",
       "\u001B[0;31m       N|\u001B[0m\n",
       "\u001B[0;31m           HV0005|              B02510|                NULL|2019-11-01 00:04:01|               NULL|2019-11-01 00:13:25|2019-11-01 00:24:09|          90|         164|     0.931|      644|               9.66|  0.0|0.24|     0.86|                2.75|       NULL| 0.0|      6.33|                  N|                N|                 N|               N|             N|\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n",
       "\u001B[0;31mVendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:26:58|  2019-12-01 00:41:45|            1.0|          4.2|       1.0|                 N|         142|         116|           2|       14.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        18.3|                 2.5|       NULL|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:12:08|  2019-12-01 00:12:14|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       NULL|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:25:53|  2019-12-01 00:26:04|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       NULL|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:12:03|  2019-12-01 00:33:19|            2.0|          9.4|       1.0|                 N|         138|          25|           1|       28.5|  0.5|    0.5|      10.0|         0.0|                  0.3|        39.8|                 0.0|       NULL|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:05:27|  2019-12-01 00:16:32|            2.0|          1.6|       1.0|                 N|         161|         237|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       NULL|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n",
       "\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:09:45|  2019-12-01 00:10:59|                 N|       1.0|         145|         145|            1.0|          0.0|        3.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         4.3|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:26:05|  2019-12-01 00:31:30|                 N|       1.0|          24|          41|            1.0|         0.67|        5.5|  0.5|    0.5|      2.04|         0.0|     NULL|                  0.3|        8.84|         1.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:38|                 N|       1.0|          74|          41|            1.0|         0.61|        4.5|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:26:20|  2019-12-01 00:40:19|                 N|       1.0|         255|         157|            1.0|          3.9|       14.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|        15.3|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:56|                 N|       1.0|          80|         255|            1.0|          0.5|        4.5|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n",
       "\u001B[0;31mdispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\u001B[0m\n",
       "\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n",
       "\u001B[0;31m              B00009|2019-12-01 00:47:00|2019-12-01 00:52:00|       264.0|       264.0|   NULL|                B00009|\u001B[0m\n",
       "\u001B[0;31m              B00009|2019-12-01 00:27:00|2019-12-01 00:44:00|       264.0|       264.0|   NULL|                B00009|\u001B[0m\n",
       "\u001B[0;31m              B00014|2019-12-01 00:42:18|2019-12-01 01:31:31|       264.0|       264.0|   NULL|                B00014|\u001B[0m\n",
       "\u001B[0;31m              B00014|2019-12-01 00:43:08|2019-12-01 01:07:38|       264.0|       264.0|   NULL|                B00014|\u001B[0m\n",
       "\u001B[0;31m     B00021         |2019-12-01 00:52:19|2019-12-01 00:59:39|        56.0|        56.0|   NULL|       B00021         |\u001B[0m\n",
       "\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31mhvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:13:49|2019-12-01 00:14:28|2019-12-01 00:16:23|2019-12-01 00:24:47|          42|          41|      1.51|      504|               9.02|  0.0|0.21|      0.8|                 0.0|       NULL| 0.0|       5.8|                  N|                Y|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:32:44|2019-12-01 00:33:04|2019-12-01 00:36:01|2019-12-01 00:47:28|         236|         166|      2.46|      687|              17.41|  0.0|0.44|     1.55|                2.75|       NULL| 0.0|      8.35|                  N|                N|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:51:37|2019-12-01 00:53:32|2019-12-01 00:54:48|2019-12-01 01:11:15|         238|          78|       9.3|      987|               14.9|  0.0|0.39|     1.38|                 0.0|       NULL| 0.0|     18.33|                  N|                N|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02764|              B02764|2019-12-01 00:45:18|2019-12-01 00:46:31|2019-12-01 00:48:19|2019-12-01 01:00:16|         148|         125|      1.86|      717|               9.01|  0.0|0.23|     0.87|                2.75|       NULL| 0.0|      7.96|                  N|                Y|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02765|              B02765|2019-11-30 23:55:59|2019-12-01 00:06:11|2019-12-01 00:08:55|2019-12-01 00:26:26|         138|         137|      8.07|     1051|              25.68| 6.12|0.81|     2.86|                2.75|       NULL| 0.0|      23.0|                  N|                N|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mData loaded and displayed.\u001B[0m"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stderr:\u001B[0m\n\u001B[0;31mThu Jul 11 07:05:53 2024 Connection to spark from PID  1083\u001B[0m\n\u001B[0;31mThu Jul 11 07:05:53 2024 Initialized gateway on port 40959\u001B[0m\n\u001B[0;31mThu Jul 11 07:05:54 2024 Connected to spark.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stdout:\u001B[0m\n\u001B[0;31m       N|\u001B[0m\n\u001B[0;31m           HV0005|              B02510|                NULL|2019-11-01 00:04:01|               NULL|2019-11-01 00:13:25|2019-11-01 00:24:09|          90|         164|     0.931|      644|               9.66|  0.0|0.24|     0.86|                2.75|       NULL| 0.0|      6.33|                  N|                N|                 N|               N|             N|\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n\u001B[0;31mVendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:26:58|  2019-12-01 00:41:45|            1.0|          4.2|       1.0|                 N|         142|         116|           2|       14.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        18.3|                 2.5|       NULL|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:12:08|  2019-12-01 00:12:14|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       NULL|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:25:53|  2019-12-01 00:26:04|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       NULL|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:12:03|  2019-12-01 00:33:19|            2.0|          9.4|       1.0|                 N|         138|          25|           1|       28.5|  0.5|    0.5|      10.0|         0.0|                  0.3|        39.8|                 0.0|       NULL|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:05:27|  2019-12-01 00:16:32|            2.0|          1.6|       1.0|                 N|         161|         237|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       NULL|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:09:45|  2019-12-01 00:10:59|                 N|       1.0|         145|         145|            1.0|          0.0|        3.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         4.3|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:26:05|  2019-12-01 00:31:30|                 N|       1.0|          24|          41|            1.0|         0.67|        5.5|  0.5|    0.5|      2.04|         0.0|     NULL|                  0.3|        8.84|         1.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:38|                 N|       1.0|          74|          41|            1.0|         0.61|        4.5|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:26:20|  2019-12-01 00:40:19|                 N|       1.0|         255|         157|            1.0|          3.9|       14.0|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|        15.3|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:56|                 N|       1.0|          80|         255|            1.0|          0.5|        4.5|  0.5|    0.5|       0.0|         0.0|     NULL|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n\u001B[0;31mdispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\u001B[0m\n\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n\u001B[0;31m              B00009|2019-12-01 00:47:00|2019-12-01 00:52:00|       264.0|       264.0|   NULL|                B00009|\u001B[0m\n\u001B[0;31m              B00009|2019-12-01 00:27:00|2019-12-01 00:44:00|       264.0|       264.0|   NULL|                B00009|\u001B[0m\n\u001B[0;31m              B00014|2019-12-01 00:42:18|2019-12-01 01:31:31|       264.0|       264.0|   NULL|                B00014|\u001B[0m\n\u001B[0;31m              B00014|2019-12-01 00:43:08|2019-12-01 01:07:38|       264.0|       264.0|   NULL|                B00014|\u001B[0m\n\u001B[0;31m     B00021         |2019-12-01 00:52:19|2019-12-01 00:59:39|        56.0|        56.0|   NULL|       B00021         |\u001B[0m\n\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31mhvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:13:49|2019-12-01 00:14:28|2019-12-01 00:16:23|2019-12-01 00:24:47|          42|          41|      1.51|      504|               9.02|  0.0|0.21|      0.8|                 0.0|       NULL| 0.0|       5.8|                  N|                Y|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:32:44|2019-12-01 00:33:04|2019-12-01 00:36:01|2019-12-01 00:47:28|         236|         166|      2.46|      687|              17.41|  0.0|0.44|     1.55|                2.75|       NULL| 0.0|      8.35|                  N|                N|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:51:37|2019-12-01 00:53:32|2019-12-01 00:54:48|2019-12-01 01:11:15|         238|          78|       9.3|      987|               14.9|  0.0|0.39|     1.38|                 0.0|       NULL| 0.0|     18.33|                  N|                N|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02764|              B02764|2019-12-01 00:45:18|2019-12-01 00:46:31|2019-12-01 00:48:19|2019-12-01 01:00:16|         148|         125|      1.86|      717|               9.01|  0.0|0.23|     0.87|                2.75|       NULL| 0.0|      7.96|                  N|                Y|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02765|              B02765|2019-11-30 23:55:59|2019-12-01 00:06:11|2019-12-01 00:08:55|2019-12-01 00:26:26|         138|         137|      8.07|     1051|              25.68| 6.12|0.81|     2.86|                2.75|       NULL| 0.0|      23.0|                  N|                N|                  |               N|             N|\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mData loaded and displayed.\u001B[0m",
       "errorSummary": "<span class='ansi-red-fg'>Fatal error</span>: The Python kernel is unresponsive.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# load the Parquet file\n",
    "df=pd.read_parquet('/tmp/nyc_taxi_data_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc305018-3d1f-4a4b-97f1-d7663c5312a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove trips with missing or corrupt data\n",
    "# Assuming 'pickup_datetime', 'dropoff_datetime', 'trip_distance', and 'fare_amount' are essential columns\n",
    "df = df.dropna(subset=['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'fare_amount'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a373db27-b3a7-41a5-bcdb-007ec1e9f0fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert datetime columns to datetime objects\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d905455-6c73-484d-8dab-f132986bfa82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Derive new columns: trip duration in minutes and average speed in miles per hour\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)  # converting duration to hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5da4a053-e2df-42e7-9a3c-28e071d1e432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate data to calculate total trips and average fare per day\n",
    "df['day'] = df['pickup_datetime'].dt.date\n",
    "aggregated_data = df.groupby('day').agg(\n",
    "    total_trips=('day', 'size'),\n",
    "    average_fare=('fare_amount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Display the aggregated data\n",
    "print(aggregated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e3d55ac-2f3a-4174-8b18-47c103057831",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab80cb2c-3628-4d99-bf22-b0198226f449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stderr:\u001B[0m\n",
       "\u001B[0;31mWed Jul 10 06:29:22 2024 Connection to spark from PID  878\u001B[0m\n",
       "\u001B[0;31mWed Jul 10 06:29:22 2024 Initialized gateway on port 46489\u001B[0m\n",
       "\u001B[0;31mWed Jul 10 06:29:23 2024 Connected to spark.\u001B[0m\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mLast messages on stdout:\u001B[0m\n",
       "\u001B[0;31m       N|\u001B[0m\n",
       "\u001B[0;31m           HV0005|              B02510|                null|2019-11-01 00:04:01|               null|2019-11-01 00:13:25|2019-11-01 00:24:09|          90|         164|     0.931|      644|               9.66|  0.0|0.24|     0.86|                2.75|       null| 0.0|      6.33|                  N|                N|                 N|               N|             N|\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n",
       "\u001B[0;31mVendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:26:58|  2019-12-01 00:41:45|            1.0|          4.2|       1.0|                 N|         142|         116|           2|       14.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        18.3|                 2.5|       null|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:12:08|  2019-12-01 00:12:14|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:25:53|  2019-12-01 00:26:04|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:12:03|  2019-12-01 00:33:19|            2.0|          9.4|       1.0|                 N|         138|          25|           1|       28.5|  0.5|    0.5|      10.0|         0.0|                  0.3|        39.8|                 0.0|       null|\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:05:27|  2019-12-01 00:16:32|            2.0|          1.6|       1.0|                 N|         161|         237|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       null|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n",
       "\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n",
       "\u001B[0;31m       1| 2019-12-01 00:09:45|  2019-12-01 00:10:59|                 N|       1.0|         145|         145|            1.0|          0.0|        3.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         4.3|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:26:05|  2019-12-01 00:31:30|                 N|       1.0|          24|          41|            1.0|         0.67|        5.5|  0.5|    0.5|      2.04|         0.0|     null|                  0.3|        8.84|         1.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:38|                 N|       1.0|          74|          41|            1.0|         0.61|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:26:20|  2019-12-01 00:40:19|                 N|       1.0|         255|         157|            1.0|          3.9|       14.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:56|                 N|       1.0|          80|         255|            1.0|          0.5|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n",
       "\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n",
       "\u001B[0;31mdispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\u001B[0m\n",
       "\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n",
       "\u001B[0;31m              B00009|2019-12-01 00:47:00|2019-12-01 00:52:00|       264.0|       264.0|   null|                B00009|\u001B[0m\n",
       "\u001B[0;31m              B00009|2019-12-01 00:27:00|2019-12-01 00:44:00|       264.0|       264.0|   null|                B00009|\u001B[0m\n",
       "\u001B[0;31m              B00014|2019-12-01 00:42:18|2019-12-01 01:31:31|       264.0|       264.0|   null|                B00014|\u001B[0m\n",
       "\u001B[0;31m              B00014|2019-12-01 00:43:08|2019-12-01 01:07:38|       264.0|       264.0|   null|                B00014|\u001B[0m\n",
       "\u001B[0;31m     B00021         |2019-12-01 00:52:19|2019-12-01 00:59:39|        56.0|        56.0|   null|       B00021         |\u001B[0m\n",
       "\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31mhvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:13:49|2019-12-01 00:14:28|2019-12-01 00:16:23|2019-12-01 00:24:47|          42|          41|      1.51|      504|               9.02|  0.0|0.21|      0.8|                 0.0|       null| 0.0|       5.8|                  N|                Y|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:32:44|2019-12-01 00:33:04|2019-12-01 00:36:01|2019-12-01 00:47:28|         236|         166|      2.46|      687|              17.41|  0.0|0.44|     1.55|                2.75|       null| 0.0|      8.35|                  N|                N|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:51:37|2019-12-01 00:53:32|2019-12-01 00:54:48|2019-12-01 01:11:15|         238|          78|       9.3|      987|               14.9|  0.0|0.39|     1.38|                 0.0|       null| 0.0|     18.33|                  N|                N|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02764|              B02764|2019-12-01 00:45:18|2019-12-01 00:46:31|2019-12-01 00:48:19|2019-12-01 01:00:16|         148|         125|      1.86|      717|               9.01|  0.0|0.23|     0.87|                2.75|       null| 0.0|      7.96|                  N|                Y|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m           HV0003|              B02765|              B02765|2019-11-30 23:55:59|2019-12-01 00:06:11|2019-12-01 00:08:55|2019-12-01 00:26:26|         138|         137|      8.07|     1051|              25.68| 6.12|0.81|     2.86|                2.75|       null| 0.0|      23.0|                  N|                N|                  |               N|             N|\u001B[0m\n",
       "\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n",
       "\u001B[0;31monly showing top 5 rows\u001B[0m\n",
       "\u001B[0;31m\u001B[0m\n",
       "\u001B[0;31mData loaded and displayed.\u001B[0m"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mThe Python process exited with exit code 137 (SIGKILL: Killed). This may have been caused by an OOM error. Check your command's memory usage.\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mThe last 10 KB of the process's stderr and stdout can be found below. See driver logs for full logs.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stderr:\u001B[0m\n\u001B[0;31mWed Jul 10 06:29:22 2024 Connection to spark from PID  878\u001B[0m\n\u001B[0;31mWed Jul 10 06:29:22 2024 Initialized gateway on port 46489\u001B[0m\n\u001B[0;31mWed Jul 10 06:29:23 2024 Connected to spark.\u001B[0m\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mLast messages on stdout:\u001B[0m\n\u001B[0;31m       N|\u001B[0m\n\u001B[0;31m           HV0005|              B02510|                null|2019-11-01 00:04:01|               null|2019-11-01 00:13:25|2019-11-01 00:24:09|          90|         164|     0.931|      644|               9.66|  0.0|0.24|     0.86|                2.75|       null| 0.0|      6.33|                  N|                N|                 N|               N|             N|\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n\u001B[0;31mVendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:26:58|  2019-12-01 00:41:45|            1.0|          4.2|       1.0|                 N|         142|         116|           2|       14.5|  3.0|    0.5|       0.0|         0.0|                  0.3|        18.3|                 2.5|       null|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:12:08|  2019-12-01 00:12:14|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:25:53|  2019-12-01 00:26:04|            1.0|          0.0|       1.0|                 N|         145|         145|           2|        2.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         3.8|                 0.0|       null|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:12:03|  2019-12-01 00:33:19|            2.0|          9.4|       1.0|                 N|         138|          25|           1|       28.5|  0.5|    0.5|      10.0|         0.0|                  0.3|        39.8|                 0.0|       null|\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:05:27|  2019-12-01 00:16:32|            2.0|          1.6|       1.0|                 N|         161|         237|           2|        9.0|  3.0|    0.5|       0.0|         0.0|                  0.3|        12.8|                 2.5|       null|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n\u001B[0;31mVendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n\u001B[0;31m       1| 2019-12-01 00:09:45|  2019-12-01 00:10:59|                 N|       1.0|         145|         145|            1.0|          0.0|        3.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         4.3|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:26:05|  2019-12-01 00:31:30|                 N|       1.0|          24|          41|            1.0|         0.67|        5.5|  0.5|    0.5|      2.04|         0.0|     null|                  0.3|        8.84|         1.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:38|                 N|       1.0|          74|          41|            1.0|         0.61|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:26:20|  2019-12-01 00:40:19|                 N|       1.0|         255|         157|            1.0|          3.9|       14.0|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|        15.3|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m       2| 2019-12-01 00:56:36|  2019-12-01 00:59:56|                 N|       1.0|          80|         255|            1.0|          0.5|        4.5|  0.5|    0.5|       0.0|         0.0|     null|                  0.3|         5.8|         2.0|      1.0|                 0.0|\u001B[0m\n\u001B[0;31m+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n\u001B[0;31mdispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\u001B[0m\n\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n\u001B[0;31m              B00009|2019-12-01 00:47:00|2019-12-01 00:52:00|       264.0|       264.0|   null|                B00009|\u001B[0m\n\u001B[0;31m              B00009|2019-12-01 00:27:00|2019-12-01 00:44:00|       264.0|       264.0|   null|                B00009|\u001B[0m\n\u001B[0;31m              B00014|2019-12-01 00:42:18|2019-12-01 01:31:31|       264.0|       264.0|   null|                B00014|\u001B[0m\n\u001B[0;31m              B00014|2019-12-01 00:43:08|2019-12-01 01:07:38|       264.0|       264.0|   null|                B00014|\u001B[0m\n\u001B[0;31m     B00021         |2019-12-01 00:52:19|2019-12-01 00:59:39|        56.0|        56.0|   null|       B00021         |\u001B[0m\n\u001B[0;31m+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31mhvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:13:49|2019-12-01 00:14:28|2019-12-01 00:16:23|2019-12-01 00:24:47|          42|          41|      1.51|      504|               9.02|  0.0|0.21|      0.8|                 0.0|       null| 0.0|       5.8|                  N|                Y|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:32:44|2019-12-01 00:33:04|2019-12-01 00:36:01|2019-12-01 00:47:28|         236|         166|      2.46|      687|              17.41|  0.0|0.44|     1.55|                2.75|       null| 0.0|      8.35|                  N|                N|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02884|              B02884|2019-12-01 00:51:37|2019-12-01 00:53:32|2019-12-01 00:54:48|2019-12-01 01:11:15|         238|          78|       9.3|      987|               14.9|  0.0|0.39|     1.38|                 0.0|       null| 0.0|     18.33|                  N|                N|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02764|              B02764|2019-12-01 00:45:18|2019-12-01 00:46:31|2019-12-01 00:48:19|2019-12-01 01:00:16|         148|         125|      1.86|      717|               9.01|  0.0|0.23|     0.87|                2.75|       null| 0.0|      7.96|                  N|                Y|                  |               N|             N|\u001B[0m\n\u001B[0;31m           HV0003|              B02765|              B02765|2019-11-30 23:55:59|2019-12-01 00:06:11|2019-12-01 00:08:55|2019-12-01 00:26:26|         138|         137|      8.07|     1051|              25.68| 6.12|0.81|     2.86|                2.75|       null| 0.0|      23.0|                  N|                N|                  |               N|             N|\u001B[0m\n\u001B[0;31m+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\u001B[0m\n\u001B[0;31monly showing top 5 rows\u001B[0m\n\u001B[0;31m\u001B[0m\n\u001B[0;31mData loaded and displayed.\u001B[0m",
       "errorSummary": "<span class='ansi-red-fg'>Fatal error</span>: The Python kernel is unresponsive.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('/tmp/nyc_taxi_data_2019')\n",
    "\n",
    "# Remove trips with missing or corrupt data\n",
    "# Assuming 'pickup_datetime', 'dropoff_datetime', 'trip_distance', and 'fare_amount' are essential columns\n",
    "df = df.dropna(subset=['pickup_datetime', 'dropoff_datetime', 'trip_distance', 'fare_amount'])\n",
    "\n",
    "# Convert datetime columns to datetime objects\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "\n",
    "# Derive new columns: trip duration in minutes and average speed in miles per hour\n",
    "df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)  # converting duration to hours\n",
    "\n",
    "# Aggregate data to calculate total trips and average fare per day\n",
    "df['day'] = df['pickup_datetime'].dt.date\n",
    "aggregated_data = df.groupby('day').agg(\n",
    "    total_trips=('day', 'size'),\n",
    "    average_fare=('fare_amount', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Display the aggregated data\n",
    "print(aggregated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69fa2337-8248-4692-8d46-1c61212b145b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-712169100413798>:67\u001B[0m\n",
       "\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m     66\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 67\u001B[0m     process_all_files(data_folder)\n",
       "\n",
       "File \u001B[0;32m<command-712169100413798>:34\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n",
       "\u001B[1;32m     31\u001B[0m file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_folder, file_name)\n",
       "\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Load the Parquet file into a Spark DataFrame\u001B[39;00m\n",
       "\u001B[0;32m---> 34\u001B[0m sdf \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Convert Spark DataFrame to Pandas DataFrame\u001B[39;00m\n",
       "\u001B[1;32m     37\u001B[0m pdf \u001B[38;5;241m=\u001B[39m sdf\u001B[38;5;241m.\u001B[39mtoPandas()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:533\u001B[0m, in \u001B[0;36mDataFrameReader.parquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n",
       "\u001B[1;32m    522\u001B[0m int96RebaseMode \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint96RebaseMode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
       "\u001B[1;32m    523\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n",
       "\u001B[1;32m    524\u001B[0m     mergeSchema\u001B[38;5;241m=\u001B[39mmergeSchema,\n",
       "\u001B[1;32m    525\u001B[0m     pathGlobFilter\u001B[38;5;241m=\u001B[39mpathGlobFilter,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    530\u001B[0m     int96RebaseMode\u001B[38;5;241m=\u001B[39mint96RebaseMode,\n",
       "\u001B[1;32m    531\u001B[0m )\n",
       "\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/nyc_taxi_data_2019/green_tripdata_2019-09.parquet."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-712169100413798>:67\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     66\u001B[0m     data_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/nyc_taxi_data_2019\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 67\u001B[0m     process_all_files(data_folder)\n\nFile \u001B[0;32m<command-712169100413798>:34\u001B[0m, in \u001B[0;36mprocess_all_files\u001B[0;34m(data_folder)\u001B[0m\n\u001B[1;32m     31\u001B[0m file_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_folder, file_name)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;66;03m# Load the Parquet file into a Spark DataFrame\u001B[39;00m\n\u001B[0;32m---> 34\u001B[0m sdf \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Convert Spark DataFrame to Pandas DataFrame\u001B[39;00m\n\u001B[1;32m     37\u001B[0m pdf \u001B[38;5;241m=\u001B[39m sdf\u001B[38;5;241m.\u001B[39mtoPandas()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:533\u001B[0m, in \u001B[0;36mDataFrameReader.parquet\u001B[0;34m(self, *paths, **options)\u001B[0m\n\u001B[1;32m    522\u001B[0m int96RebaseMode \u001B[38;5;241m=\u001B[39m options\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mint96RebaseMode\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[1;32m    524\u001B[0m     mergeSchema\u001B[38;5;241m=\u001B[39mmergeSchema,\n\u001B[1;32m    525\u001B[0m     pathGlobFilter\u001B[38;5;241m=\u001B[39mpathGlobFilter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    530\u001B[0m     int96RebaseMode\u001B[38;5;241m=\u001B[39mint96RebaseMode,\n\u001B[1;32m    531\u001B[0m )\n\u001B[0;32m--> 533\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_to_seq\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_spark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpaths\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/nyc_taxi_data_2019/green_tripdata_2019-09.parquet.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [PATH_NOT_FOUND] Path does not exist: dbfs:/tmp/nyc_taxi_data_2019/green_tripdata_2019-09.parquet.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def clean_and_transform(pdf):\n",
    "    # Remove trips with missing or corrupt data\n",
    "    pdf.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    pdf['pickup_datetime'] = pd.to_datetime(pdf['tpep_pickup_datetime'])\n",
    "    pdf['dropoff_datetime'] = pd.to_datetime(pdf['tpep_dropoff_datetime'])\n",
    "    pdf['trip_duration'] = (pdf['dropoff_datetime'] - pdf['pickup_datetime']).dt.total_seconds() / 60\n",
    "    pdf['average_speed'] = pdf['trip_distance'] / (pdf['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    pdf['date'] = pdf['pickup_datetime'].dt.date\n",
    "    daily_aggregates = pdf.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return pdf, daily_aggregates\n",
    "\n",
    "def process_all_files(data_folder):\n",
    "    spark = SparkSession.builder.appName(\"NYC Taxi Data Processing\").getOrCreate()\n",
    "    \n",
    "    processed_data = []\n",
    "    daily_aggregates_list = []\n",
    "    \n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        \n",
    "        # Load the Parquet file into a Spark DataFrame\n",
    "        sdf = spark.read.parquet(file_path)\n",
    "        \n",
    "        # Convert Spark DataFrame to Pandas DataFrame\n",
    "        pdf = sdf.toPandas()\n",
    "        \n",
    "        # Clean and transform the data\n",
    "        cleaned_pdf, daily_agg = clean_and_transform(pdf)\n",
    "        \n",
    "        # Convert cleaned Pandas DataFrame back to Spark DataFrame\n",
    "        cleaned_sdf = spark.createDataFrame(cleaned_pdf)\n",
    "        \n",
    "        # Collect daily aggregates\n",
    "        daily_aggregates_list.append(daily_agg)\n",
    "        \n",
    "        # Append the cleaned Spark DataFrame to the list\n",
    "        processed_data.append(cleaned_sdf)\n",
    "    \n",
    "    # Concatenate all cleaned Spark DataFrames\n",
    "    if processed_data:\n",
    "        final_sdf = processed_data[0]\n",
    "        for sdf in processed_data[1:]:\n",
    "            final_sdf = final_sdf.union(sdf)\n",
    "    \n",
    "    # Save the final cleaned data\n",
    "    final_sdf.write.parquet(\"/dbfs/tmp/processed_data.parquet\", mode=\"overwrite\")\n",
    "    \n",
    "    # Concatenate all daily aggregates\n",
    "    if daily_aggregates_list:\n",
    "        final_daily_aggregates = pd.concat(daily_aggregates_list)\n",
    "        final_daily_aggregates.to_parquet(\"/dbfs/tmp/daily_aggregates.parquet\", index=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/tmp/nyc_taxi_data_2019\"\n",
    "    process_all_files(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "664c3dc8-87d5-48b4-b7e4-e9767b30fa6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def clean_and_transform(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Remove trips with missing or corrupt data\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Derive new columns\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    df['trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 60\n",
    "    df['average_speed'] = df['trip_distance'] / (df['trip_duration'] / 60)\n",
    "    \n",
    "    # Aggregate data\n",
    "    df['date'] = df['pickup_datetime'].dt.date\n",
    "    daily_aggregates = df.groupby('date').agg({\n",
    "        'trip_duration': 'count',\n",
    "        'fare_amount': 'mean'\n",
    "    }).rename(columns={'trip_duration': 'total_trips', 'fare_amount': 'average_fare'})\n",
    "    \n",
    "    return df, daily_aggregates\n",
    "\n",
    "def process_all_files(data_folder):\n",
    "    processed_data = []\n",
    "    daily_aggregates = []\n",
    "    \n",
    "    for file_name in os.listdir(data_folder):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        df, daily_agg = clean_and_transform(file_path)\n",
    "        processed_data.append(df)\n",
    "        daily_aggregates.append(daily_agg)\n",
    "    \n",
    "    return pd.concat(processed_data), pd.concat(daily_aggregates)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_folder = \"/dbfs/tmp/2019\"\n",
    "    processed_data, daily_aggregates = process_all_files(data_folder)\n",
    "    processed_data.to_parquet(\"/dbfs/tmp/processed_data.parquet\", index=False)\n",
    "    daily_aggregates.to_parquet(\"/dbfs/tmp/daily_aggregates.parquet\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b36ad7d2-d9d3-4997-8dd7-3fed0a1c5bd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read and display processed data\n",
    "processed_data = pd.read_parquet(\"/dbfs/tmp/processed_data.parquet\")\n",
    "daily_aggregates = pd.read_parquet(\"/dbfs/tmp/daily_aggregates.parquet\")\n",
    "\n",
    "print(processed_data.head())\n",
    "print(daily_aggregates.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d784c1d-9380-4eb3-b99d-b739cca46b70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime TEXT,\n",
    "    dropoff_datetime TEXT,\n",
    "    trip_duration REAL,\n",
    "    trip_distance REAL,\n",
    "    fare_amount REAL,\n",
    "    passenger_count INTEGER,\n",
    "    average_speed REAL\n",
    ");\n",
    "\n",
    "CREATE TABLE daily_aggregates (\n",
    "    date TEXT PRIMARY KEY,\n",
    "    total_trips INTEGER,\n",
    "    average_fare REAL\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88121891-73ef-42c5-8af7-21845aa9837a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_to_sqlite(processed_data_file, daily_aggregates_file, db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    processed_data = pd.read_csv(processed_data_file)\n",
    "    daily_aggregates = pd.read_csv(daily_aggregates_file)\n",
    "    \n",
    "    processed_data.to_sql('trips', conn, if_exists='replace', index=False)\n",
    "    daily_aggregates.to_sql('daily_aggregates', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_data_to_sqlite(\"processed_data.csv\", \"daily_aggregates.csv\", \"ny_taxi.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db8c1cf-36cc-4aad-94a4-d4cf28468fad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Peak hours for taxi usage\n",
    "SELECT strftime('%H', pickup_datetime) as hour, COUNT(*) as total_trips\n",
    "FROM trips\n",
    "GROUP BY hour\n",
    "ORDER BY total_trips DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- Passenger count effect on trip fare\n",
    "SELECT passenger_count, AVG(fare_amount) as average_fare\n",
    "FROM trips\n",
    "GROUP BY passenger_count;\n",
    "\n",
    "-- Trends in usage over the year\n",
    "SELECT date, total_trips, average_fare\n",
    "FROM daily_aggregates\n",
    "ORDER BY date;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3454dcaa-8400-4c2a-a72a-35d73e47a4a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def create_visualizations(db_name):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    \n",
    "    # Peak hours visualization\n",
    "    query = \"SELECT strftime('%H', pickup_datetime) as hour, COUNT(*) as total_trips FROM trips GROUP BY hour ORDER BY total_trips DESC LIMIT 10;\"\n",
    "    peak_hours = pd.read_sql(query, conn)\n",
    "    sns.barplot(x='hour', y='total_trips', data=peak_hours)\n",
    "    plt.title(\"Peak Hours for Taxi Usage\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Passenger count effect on fare\n",
    "    query = \"SELECT passenger_count, AVG(fare_amount) as average_fare FROM trips GROUP BY passenger_count;\"\n",
    "    passenger_fares = pd.read_sql(query, conn)\n",
    "    sns.barplot(x='passenger_count', y='average_fare', data=passenger_fares)\n",
    "    plt.title(\"Effect of Passenger Count on Fare\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Trends over the year\n",
    "    query = \"SELECT date, total_trips, average_fare FROM daily_aggregates ORDER BY date;\"\n",
    "    trends = pd.read_sql(query, conn)\n",
    "    trends['date'] = pd.to_datetime(trends['date'])\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "    ax1.plot(trends['date'], trends['total_trips'], 'g-')\n",
    "    ax2.plot(trends['date'], trends['average_fare'], 'b-')\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Total Trips', color='g')\n",
    "    ax2.set_ylabel('Average Fare', color='b')\n",
    "    plt.title(\"Trends in Taxi Usage Over the Year\")\n",
    "    plt.show()\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_visualizations(\"ny_taxi.db\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09e863a8-3f96-43a9-bd19-3404d6c41d37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# New York Taxi Data Processing\n",
    "\n",
    "## Project Overview\n",
    "This project processes New York Taxi Trip data for the year 2019 to derive analytical insights and load the processed data into a SQLite database for further analysis.\n",
    "\n",
    "## Environment Setup\n",
    "- Python 3.7+\n",
    "- Pandas\n",
    "- Requests\n",
    "- TQDM\n",
    "- SQLite3\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "\n",
    "## Running the Project\n",
    "1. Clone the repository.\n",
    "2. Run the data extraction script:\n",
    "    ```bash\n",
    "    python data_extraction.py\n",
    "    ```\n",
    "3. Run the data processing script:\n",
    "    ```bash\n",
    "    python data_processing.py\n",
    "    ```\n",
    "4. Load the processed data into SQLite:\n",
    "    ```bash\n",
    "    python data_loading.py\n",
    "    ```\n",
    "5. Generate visualizations:\n",
    "    ```bash\n",
    "    python data_analysis.py\n",
    "    ```\n",
    "\n",
    "## Data Analysis\n",
    "- Peak hours for taxi usage.\n",
    "- Effect of passenger count on trip fare.\n",
    "- Trends in taxi usage over the year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c143f8a7-1bf4-482d-87e2-de313ee07015",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "D2k_assignment (1)",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
